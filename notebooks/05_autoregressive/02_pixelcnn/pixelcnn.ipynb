{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206a93a2-e9b0-4ea2-a43f-696faa83ea03",
   "metadata": {},
   "source": [
    "# ðŸ‘¾ PixelCNN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9e216-7e84-4f5b-a2db-26aca3bea464",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own PixelCNN on the fashion MNIST dataset from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e6bbc-6f3b-48ac-a4f3-fde6f739f0ca",
   "metadata": {},
   "source": [
    "The code has been adapted from the excellent [PixelCNN tutorial](https://keras.io/examples/generative/pixelcnn/) created by ADMoreau, available on the Keras website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6acebfa8-4546-41fd-adaa-2307c65b1b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers, callbacks\n",
    "\n",
    "from notebooks.utils import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8543166d-f4c7-43f8-a452-21ccbf2a0496",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "444d84de-2843-40d6-8e2e-93691a5393ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 16\n",
    "PIXEL_LEVELS = 4\n",
    "N_FILTERS = 128\n",
    "RESIDUAL_BLOCKS = 5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65dac68-d20b-4ed9-a136-eed57095ce4f",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ed0fc56-d1b0-4d42-b029-f4198f78e666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 3s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "(x_train, _), (_, _) = datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b667e78c-8fa7-4e5b-a2c0-69e50166ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess(imgs_int):\n",
    "    imgs_int = np.expand_dims(imgs_int, -1)\n",
    "    imgs_int = tf.image.resize(imgs_int, (IMAGE_SIZE, IMAGE_SIZE)).numpy()\n",
    "    imgs_int = (imgs_int / (256 / PIXEL_LEVELS)).astype(int)\n",
    "    imgs = imgs_int.astype(\"float32\")\n",
    "    imgs = imgs / PIXEL_LEVELS\n",
    "    return imgs, imgs_int\n",
    "\n",
    "\n",
    "input_data, output_data = preprocess(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c2b304-8385-4931-8291-9b7cc462c95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show some items of clothing from the training set\n",
    "display(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccd5cb2-8c7b-4667-8adb-4902f3fa60cf",
   "metadata": {},
   "source": [
    "## 2. Build the PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "847050a5-e4e6-4134-9bfc-c690cb8cb44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first layer is the PixelCNN layer. This layer simply\n",
    "# builds on the 2D convolutional layer, but includes masking.\n",
    "class MaskedConv2D(layers.Layer):\n",
    "    def __init__(self, mask_type, **kwargs):\n",
    "        super(MaskedConv2D, self).__init__()\n",
    "        self.mask_type = mask_type\n",
    "        self.conv = layers.Conv2D(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Build the conv2d layer to initialize kernel variables\n",
    "        self.conv.build(input_shape)\n",
    "        # Use the initialized kernel to create the mask\n",
    "        kernel_shape = self.conv.kernel.get_shape()\n",
    "        self.mask = np.zeros(shape=kernel_shape)\n",
    "        self.mask[: kernel_shape[0] // 2, ...] = 1.0\n",
    "        self.mask[kernel_shape[0] // 2, : kernel_shape[1] // 2, ...] = 1.0\n",
    "        if self.mask_type == \"B\":\n",
    "            self.mask[kernel_shape[0] // 2, kernel_shape[1] // 2, ...] = 1.0\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.conv.kernel.assign(self.conv.kernel * self.mask)\n",
    "        return self.conv(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a52f7795-790e-47b0-b724-80be3e3c3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(layers.Layer):\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "        self.conv1 = layers.Conv2D(\n",
    "            filters=filters // 2, kernel_size=1, activation=\"relu\"\n",
    "        )\n",
    "        self.pixel_conv = MaskedConv2D(\n",
    "            mask_type=\"B\",\n",
    "            filters=filters // 2,\n",
    "            kernel_size=3,\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.conv2 = layers.Conv2D(\n",
    "            filters=filters, kernel_size=1, activation=\"relu\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pixel_conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return layers.add([inputs, x])\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b4508f-84de-42a9-a77f-950fb493db13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 16, 16, 1)]       0         \n",
      "                                                                 \n",
      " masked_conv2d (MaskedConv2D  (None, 16, 16, 128)      6400      \n",
      " )                                                               \n",
      "                                                                 \n",
      " residual_block (ResidualBlo  (None, 16, 16, 128)      53504     \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " residual_block_1 (ResidualB  (None, 16, 16, 128)      53504     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " residual_block_2 (ResidualB  (None, 16, 16, 128)      53504     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " residual_block_3 (ResidualB  (None, 16, 16, 128)      53504     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " residual_block_4 (ResidualB  (None, 16, 16, 128)      53504     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " masked_conv2d_6 (MaskedConv  (None, 16, 16, 128)      16512     \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " masked_conv2d_7 (MaskedConv  (None, 16, 16, 128)      16512     \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 16, 16, 4)         516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 307,460\n",
      "Trainable params: 307,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "x = MaskedConv2D(\n",
    "    mask_type=\"A\",\n",
    "    filters=N_FILTERS,\n",
    "    kernel_size=7,\n",
    "    activation=\"relu\",\n",
    "    padding=\"same\",\n",
    ")(inputs)\n",
    "\n",
    "for _ in range(RESIDUAL_BLOCKS):\n",
    "    x = ResidualBlock(filters=N_FILTERS)(x)\n",
    "\n",
    "for _ in range(2):\n",
    "    x = MaskedConv2D(\n",
    "        mask_type=\"B\",\n",
    "        filters=N_FILTERS,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        activation=\"relu\",\n",
    "        padding=\"valid\",\n",
    "    )(x)\n",
    "\n",
    "out = layers.Conv2D(\n",
    "    filters=PIXEL_LEVELS,\n",
    "    kernel_size=1,\n",
    "    strides=1,\n",
    "    activation=\"softmax\",\n",
    "    padding=\"valid\",\n",
    ")(x)\n",
    "\n",
    "pixel_cnn = models.Model(inputs, out)\n",
    "pixel_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b5ffa-67a3-4b15-a342-eb1eed5e87ac",
   "metadata": {},
   "source": [
    "## 3. Train the PixelCNN <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7204789a-2ad3-48bf-b7e8-00d4cab10d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(learning_rate=0.0005)\n",
    "pixel_cnn.compile(optimizer=adam, loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09d327fc-aff8-40e6-b390-d1bff4c06ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "\n",
    "class ImageGenerator(callbacks.Callback):\n",
    "    def __init__(self, num_img):\n",
    "        self.num_img = num_img\n",
    "\n",
    "    def sample_from(self, probs, temperature):  # <2>\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "        return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "    def generate(self, temperature):\n",
    "        generated_images = np.zeros(\n",
    "            shape=(self.num_img,) + (pixel_cnn.input_shape)[1:]\n",
    "        )\n",
    "        batch, rows, cols, channels = generated_images.shape\n",
    "\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                for channel in range(channels):\n",
    "                    probs = self.model.predict(generated_images, verbose=0)[\n",
    "                        :, row, col, :\n",
    "                    ]\n",
    "                    generated_images[:, row, col, channel] = [\n",
    "                        self.sample_from(x, temperature) for x in probs\n",
    "                    ]\n",
    "                    generated_images[:, row, col, channel] /= PIXEL_LEVELS\n",
    "\n",
    "        return generated_images\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        generated_images = self.generate(temperature=1.0)\n",
    "        display(\n",
    "            generated_images,\n",
    "            save_to=\"./output/generated_img_%03d.png\" % (epoch),\n",
    "        )\n",
    "\n",
    "\n",
    "img_generator_callback = ImageGenerator(num_img=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85231056-d4a4-4897-ab91-065325a18d93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pixel_cnn.fit(\n",
    "    input_data,\n",
    "    output_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[tensorboard_callback, img_generator_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb4fa72-dd2d-44c1-ad18-9c965060683e",
   "metadata": {},
   "source": [
    "## 4. Generate images <a name=\"generate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bbd4643-be09-49ba-b7bc-a524a2f00806",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = img_generator_callback.generate(temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52cadb4b-ae2c-42a9-92ac-68e2131380ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(generated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448addf-e3ca-4d31-aad8-7abb1faf9109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
